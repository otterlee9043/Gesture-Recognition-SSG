{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sskim\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "from scipy import stats\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import random\n",
    "\n",
    "fps=120\n",
    "threshold=1\n",
    "moving_avg_len=10\n",
    "filter_value = 20\n",
    "length=200\n",
    "\n",
    "\n",
    "def readFileData(file):\n",
    "    column_names = ['wmx1', 'wmy1', 'wmz1']\n",
    "\n",
    "    data = pd.read_csv(file, skiprows = 1 , names = column_names)\n",
    " \n",
    "    wx = data[\"wmx1\"]\n",
    "    wy = data[\"wmy1\"]\n",
    "    wz = data[\"wmz1\"]\n",
    "\n",
    "    records=movingavg(wx,wy,wz)\n",
    "\n",
    "    return np.dstack([wx,wy,wz])[0]\n",
    "\n",
    "def readData(directory):\n",
    "    records = []\n",
    "    labels = np.empty(0)\n",
    "    \n",
    "    allFiles = glob.glob(\"*.csv\")\n",
    "    for i,file in enumerate(allFiles):\n",
    "        fileName = os.path.basename(file)\n",
    "        print(\"file name is\", fileName,\"num is\",i)\n",
    "        (name, ext) = os.path.splitext(fileName)\n",
    "        #parts = name.split(\"_\")\n",
    "        if (True):\n",
    "            label = \"circle\"\n",
    "            fileData = readFileData(file)\n",
    "            print(\"file datat is \",fileData)\n",
    "            records.append(fileData)\n",
    "            labels = np.append(labels, label)\n",
    "\n",
    "    return (records, labels)\n",
    "\n",
    "def movingavg(x,y,z):\n",
    "    x_avg = np.zeros(moving_avg_len)/moving_avg_len\n",
    "    y_avg = np.zeros(moving_avg_len)/moving_avg_len\n",
    "    z_avg = np.zeros(moving_avg_len)/moving_avg_len\n",
    "    x_avg = np.convolve(x,x_avg,'same')\n",
    "    y_avg = np.convolve(y,y_avg,'same')\n",
    "    z_avg = np.convolve(z,z_avg,'same')\n",
    "    \n",
    "    return [x_avg,y_avg,z_avg]\n",
    "\n",
    "def splitData(records):##sungshil\n",
    "    print(\"records is \",records[0][0])\n",
    "    term=fps//4\n",
    "    record_sum=[]\n",
    "    record=[]\n",
    "    record_all=[]\n",
    "    ct=0\n",
    "    sum_now=0.0\n",
    "    sum_pre=0.0\n",
    "    start=0\n",
    "    end=0\n",
    "    \n",
    "    for i in range((len(records[0])//(term))-100):\n",
    "        sum=0.0\n",
    "        record_x=np.empty(shape=[1],dtype=float)\n",
    "        record_y=np.empty(shape=[1],dtype=float)\n",
    "        record_z=np.empty(shape=[1],dtype=float)\n",
    "        for j in range(fps):\n",
    "            record_x=np.append(record_x,records[0][term*i+j][0])\n",
    "            record_y=np.append(record_y,records[0][term*i+j][1])\n",
    "            record_z=np.append(record_z,records[0][term*i+j][2])\n",
    "        for k in range(fps-2):\n",
    "            sum=sum+(record_x[k]-record_x[k+1])**2+(record_y[k]-record_y[k+1])**2+(record_z[k]-record_z[k+1])**2\n",
    "            \n",
    "        sum_pre=sum_now\n",
    "        sum_now=sum\n",
    "        record_sum.append(sum)\n",
    "        if (sum_pre<threshold and sum_now>threshold):\n",
    "            \n",
    "            ct=ct+1\n",
    "            start=term*i\n",
    "            print(\"start\",start)\n",
    "        if (sum_pre>threshold and sum_now<threshold):\n",
    "            end=term*i  \n",
    "            print(\"end\",end)\n",
    "            print(\"step is \" ,ct)\n",
    "            print(\"length of end-start\",(end-start))\n",
    "            s=[]\n",
    "            for k in range(end-start):\n",
    "                #t=(records[0][term*i+j+k])\n",
    "                #print(\"t is\",t)\n",
    "                s.append(records[0][start+k])\n",
    "                #print(\"s is \",s)\n",
    "            record_all.append(s)\n",
    "             \n",
    "    print(\"number of patterm : \",ct)\n",
    "    print(\"average is\",np.average(record_sum))\n",
    "    print(\"number of term\",len(record_sum))\n",
    "  \n",
    "    plt.plot(record_sum[1500:2500])\n",
    "    \n",
    "    return record_all\n",
    "\n",
    "def getRecordsMaxLength(records):\n",
    "    maxLen = 0\n",
    "    for record in records:\n",
    "        if (len(record) > maxLen):\n",
    "            maxLen = len(record)\n",
    "        \n",
    "    return maxLen\n",
    "\n",
    "def shortenRecordsLen(records, length):\n",
    "    ret=[]\n",
    "    rec_len=len(records)\n",
    "    \n",
    "    for index in range(len(records)):\n",
    "        s=[]\n",
    "        record = records[index]\n",
    "        if len(record) > length and len(record)<3000:\n",
    "            for i in range(length):\n",
    "                r=[]\n",
    "                t=i*len(record)/length\n",
    "                if t%1==0:\n",
    "                    s.append(records[index][int(t)])\n",
    "                else:\n",
    "                    a=t//1\n",
    "                    s1=t-a\n",
    "                    s2=a+1-t\n",
    "                    for i in range(3):\n",
    "                        r.append(s1*records[index][int(a)][i]+s2*records[index][int(a)][i])\n",
    "                    s.append(r) \n",
    "        \n",
    "                \n",
    "        ret.append(s)\n",
    "    plt.plot\n",
    "    return ret\n",
    "\n",
    "def weight_variable(shape):#가중치 생성\n",
    "    initial = tf.truncated_normal(shape, stddev = 0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):#편향 생성\n",
    "    initial = tf.constant(0.0, shape = shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def depthwise_conv2d(x, W):\n",
    "    return tf.nn.depthwise_conv2d(x,W, [1, 1, 1, 1], padding='VALID')\n",
    "\n",
    "def apply_depthwise_conv(x,kernel_size,num_channels,depth):#각 채널마다 활성곱 수행후 ReLu활성화 함수로 pass\n",
    "    weights = weight_variable([1, kernel_size, num_channels, depth])\n",
    "    biases = bias_variable([depth * num_channels])\n",
    "    return tf.nn.relu(tf.add(depthwise_conv2d(x, weights),biases))\n",
    "    \n",
    "def apply_max_pool(x,kernel_size,stride_size):#생성된 convolution층에 pooling 적용\n",
    "    return tf.nn.max_pool(x, ksize=[1, 1, kernel_size, 1], \n",
    "                          strides=[1, 1, stride_size, 1], padding='VALID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-98fa9df7c851>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#print(\"size of divide \",len(records[0])//40)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#records=filter_data(records)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mrecords\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msplitData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshortenRecordsLen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-553a8be9c0c0>\u001b[0m in \u001b[0;36msplitData\u001b[1;34m(records)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msplitData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;31m##sungshil\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"records is \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m     \u001b[0mterm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfps\u001b[0m\u001b[1;33m//\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[0mrecord_sum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "(records,labels) = readData(\"dr\")\n",
    "labels = np.asarray(labels)\n",
    "print(len(labels))\n",
    "#print(\"size of \",len(records[0]))\n",
    "#print(\"size of divide \",len(records[0])//40)\n",
    "#records=filter_data(records)\n",
    "records=splitData(records)\n",
    "records=shortenRecordsLen(records, length)\n",
    "print(records)\n",
    "print(records[0])\n",
    "\n",
    "for i in range(len(records)):\n",
    "    plt.plot(records[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "records = np.asarray(records)\n",
    "reshaped_records = records.reshape(len(records),1,200, 3) #recoed를 1차원으로 reshape\n",
    "print(reshaped_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split = np.random.rand(len(reshaped_records)) < 0.70\n",
    "train_test_split.astype(np.int)\n",
    "print(train_test_split)\n",
    "# training과 testing set 랜덤하게 70/30 나누기\n",
    "train_x = reshaped_records[train_test_split[0]]\n",
    "print(train_x)\n",
    "train_y = labels[train_test_split[0]]\n",
    "test_x = reshaped_records[~train_test_split[0]]\n",
    "test_y = labels[~train_test_split[0]]\n",
    "print(len(train_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_height = 1\n",
    "input_width = 200\n",
    "num_labels = 6\n",
    "num_channels = 3 #?? input과 channel의 차이 무엇?\n",
    "\n",
    "batch_size = 10\n",
    "kernel_size = 60#??\n",
    "depth = 60\n",
    "num_hidden = 1000\n",
    "\n",
    "learning_rate = 0.0001\n",
    "training_epochs = 8\n",
    "\n",
    "total_batches = train_x.shape[0] // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None,input_height,input_width,num_channels])\n",
    "# input data\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "# output data\n",
    "\n",
    "c = apply_depthwise_conv(X,kernel_size,num_channels,depth) #1d convolution\n",
    "p = apply_max_pool(c,20,2) #1d Max-Pooling\n",
    "c = apply_depthwise_conv(p,6,depth*num_channels,depth//10) #1d convolution\n",
    "\n",
    "shape = c.get_shape().as_list()\n",
    "c_flat = tf.reshape(c, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "f_weights_l1 = weight_variable([shape[1] * shape[2] * depth * num_channels * (depth//10), num_hidden])\n",
    "f_biases_l1 = bias_variable([num_hidden])\n",
    "f = tf.nn.tanh(tf.add(tf.matmul(c_flat, f_weights_l1),f_biases_l1))\n",
    "\n",
    "out_weights = weight_variable([num_hidden, num_labels])\n",
    "out_biases = bias_variable([num_labels])\n",
    "y_ = tf.nn.softmax(tf.matmul(f, out_weights) + out_biases)\n",
    "# softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = -tf.reduce_sum(Y * tf.log(y_))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cost_history = np.empty(shape=[1],dtype=float)\n",
    " \n",
    "with tf.Session() as session: #training, 위에서 c, p는 어떻게 apply하는 거지?\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(training_epochs):\n",
    "        for b in range(total_batches):    \n",
    "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([optimizer, loss],feed_dict={X: batch_x, Y : batch_y})\n",
    "            cost_history = np.append(cost_history,c)\n",
    "        print (\"Epoch: \",epoch,\" Training Loss: \",c,\" Training Accuracy: \",\n",
    "              session.run(accuracy, feed_dict={X: train_x, Y: train_y}))\n",
    "    \n",
    "    print (\"Testing Accuracy:\", session.run(accuracy, feed_dict={X: test_x, Y: test_y}))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
