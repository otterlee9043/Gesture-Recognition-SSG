{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sskim\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "from scipy import stats\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fps = 120\n",
    "moving_avg_len = 10\n",
    "filter_value = 20\n",
    "length = 200\n",
    "threshold_pattern=0.4\n",
    "#10.18. 목\n",
    "\n",
    "\n",
    "\n",
    "def readFileData(file):\n",
    "    column_names = ['wmx1', 'wmy1', 'wmz1']\n",
    "\n",
    "    data = pd.read_csv(file, skiprows = 1 , names = column_names)\n",
    " \n",
    "    wx = data[\"wmx1\"]\n",
    "    wy = data[\"wmy1\"]\n",
    "    wz = data[\"wmz1\"]\n",
    "    \n",
    "    return np.dstack([wx,wy,wz])[0]\n",
    "\n",
    "def readData(directory):\n",
    "    records = []\n",
    "    labels = []\n",
    "    gesture_num_records = []\n",
    "    allFiles = glob.glob(\"*.csv\")\n",
    "    for i,file in enumerate(allFiles):\n",
    "        fileName = os.path.basename(file)\n",
    "        print(\"file name is\", fileName,\"num is\",i)\n",
    "        (name, ext) = os.path.splitext(fileName)\n",
    "        print(\"name is \",name,\"ext is\",ext)\n",
    "        parts = name.split(\"_\")\n",
    "        if (True):\n",
    "            label = parts[1] #'circle', 'triangle', 'X'\n",
    "            fileData = readFileData(file)\n",
    "            print(\"fileData :\",len(fileData[1]))\n",
    "            records.append(fileData)\n",
    "            #print(\"file data is \",fileData)\n",
    "            labels.append(label)\n",
    "            gesture_num_records.append(len(fileData))\n",
    "\n",
    "    return (records, labels, gesture_num_records)\n",
    "\n",
    "def movingavg(x,y,z):\n",
    "    x_avg = np.zeros(moving_avg_len)/moving_avg_len\n",
    "    y_avg = np.zeros(moving_avg_len)/moving_avg_len\n",
    "    z_avg = np.zeros(moving_avg_len)/moving_avg_len\n",
    "    x_avg = np.convolve(x,x_avg,'same')\n",
    "    y_avg = np.convolve(y,y_avg,'same')\n",
    "    z_avg = np.convolve(z,z_avg,'same')\n",
    "    return [x_avg,y_avg,z_avg]\n",
    "\n",
    "def splitData(records):##sungshil\n",
    "    \n",
    "    term=fps // 24\n",
    "    record_sum = []\n",
    "    record = []\n",
    "    record_all = []\n",
    "    sum_now = 0\n",
    "    sum_pre = 0\n",
    "    ct = 0\n",
    "    start = 0\n",
    "    end = 0\n",
    "    num_pattern=[]\n",
    "    for t in range(len(records)):\n",
    "        ct = 0\n",
    "        record_one=[]\n",
    "        for i in range((len(records[t])//(term))-100):\n",
    "            sum = 0\n",
    "            record_x = np.empty(shape=[1],dtype=float)\n",
    "            record_y = np.empty(shape=[1],dtype=float)\n",
    "            record_z = np.empty(shape=[1],dtype=float)\n",
    "            for j in range(fps//2):\n",
    "                record_x = np.append(record_x,records[t][term*i+j][0])\n",
    "                record_y = np.append(record_y,records[t][term*i+j][1])\n",
    "                record_z = np.append(record_z,records[t][term*i+j][2])\n",
    "            for k in range(fps//2 - 2):\n",
    "                sum = sum+(record_x[k]-record_x[k+1])**2+(record_y[k]-record_y[k+1])**2+(record_z[k]-record_z[k+1])**2\n",
    "\n",
    "            sum_pre = sum_now\n",
    "            sum_now = sum\n",
    "            record_sum.append(sum)\n",
    "            if (sum_pre < threshold_pattern and sum_now > threshold_pattern):\n",
    "                start = term * i\n",
    "            if (sum_pre > threshold_pattern and sum_now < threshold_pattern):\n",
    "                end = term * i  \n",
    "                s = []\n",
    "                if (end-start)>120:\n",
    "                    ct = ct + 1\n",
    "                    #print(\">> start : \",start)\n",
    "                    #print(\" >> end : \",end,\", step : \" ,ct, \", length of end-start : \",(end-start))\n",
    "                    for k in range(end-start):\n",
    "                        #t=(records[0][term*i+j+k])\n",
    "                        #print(\"t is\",t)\n",
    "                        s.append(records[t][start+k])\n",
    "                        #print(\"s is \",s)\n",
    "                    record_one.append(s)\n",
    "        record_all.append(record_one)\n",
    "        print(\"number of patterm : \",ct)\n",
    "        num_pattern.append(ct)\n",
    "        #print(\"average is\",np.average(record_sum))\n",
    "        #plt.plot(record_sum[1500:2000])\n",
    " \n",
    "    return (record_all,num_pattern)\n",
    "\n",
    "def getRecordsMaxLength(records):\n",
    "    maxLen = 0\n",
    "    for record in records:\n",
    "        if (len(record) > maxLen):\n",
    "            maxLen = len(record)\n",
    "        \n",
    "    return maxLen\n",
    "\n",
    "def shortenRecordsLen(records, length, first_label):\n",
    "    ret1 = []\n",
    "    new_labels = []\n",
    "    for num in range(len(records)):\n",
    "        ret2 = []\n",
    "        for index in range(len(records[num])):\n",
    "            s = []\n",
    "            if (len(records[num][index])>length):\n",
    "                record = records[num][index]\n",
    "                if len(record) < 1000:\n",
    "                    new_labels.append(first_label[num])\n",
    "                    for i in range(length):\n",
    "                        ret3=[]                      \n",
    "                        t=i*(len(record)/length)\n",
    "                        if t%1 == 0:\n",
    "                            s.append(record[int(t)])\n",
    "                        else:\n",
    "                            a = t//1\n",
    "                            s1 = t-a\n",
    "                            s2 = a+1-t\n",
    "                            for j in range(3):\n",
    "                                ret3.append( s1*record[int(a)][j] + s2*record[int(a)][j] )\n",
    "                            s.append(ret3)\n",
    "                ret2.append(s)           \n",
    "                \n",
    "        ret1.append(ret2)\n",
    "    return ret1, new_labels\n",
    "\n",
    "def labeling(records, num_pattern):\n",
    "    labels = []\n",
    "    '''recods 합쳐진 다음에 num_pattern로 제스쳐 개수 계산하지 말고 records length로 바꾸기 '''\n",
    "    for i in range(num_pattern[0] + num_pattern[1] + num_pattern[2]):\n",
    "        if i < num_pattern[0] :\n",
    "            labels.append('circle')\n",
    "        elif i < num_pattern[0] + num_pattern[1]:\n",
    "            labels.append('triangle')\n",
    "        else :labels.append('X')\n",
    "    print(\"labelling - labels length : \", len(labels))\n",
    "    return labels\n",
    "\n",
    "def pattern_reshape(records,length,labels):\n",
    "    segment=[]\n",
    "    record_po=np.empty(0)\n",
    "    record_lab=[]\n",
    "    ct=0\n",
    "    for i in range(len(records)):\n",
    "        for j in range(len(records[i])): # num of gesture 횟수만큼\n",
    "            length_ary=[]\n",
    "            for k in range(len(records[i][j])): # 150회만큼\n",
    "                x=np.array(records[i][j][k][0]) # files X num of gesture X 150 X 3\n",
    "                y=np.array(records[i][j][k][1])\n",
    "                z=np.array(records[i][j][k][2])\n",
    "                length_ary.append([x,y,z])\n",
    "            if len(records[i][j])==length:\n",
    "                segment.append([length_ary,labels[ct]])\n",
    "                ct+=1\n",
    "\n",
    "    for k in range(len(segment)):\n",
    "        record_lab.append(segment[k][1])\n",
    "        for t in range(len(segment[k][0])): # x,y,z\n",
    "            record_po=np.append(record_po,segment[k][0][t])\n",
    "            \n",
    "            \n",
    "    return record_po,record_lab\n",
    "\n",
    "    \n",
    "def missing_noise(records):\n",
    "    for i in range(len(records)):\n",
    "        for j in range(len(records[i])):\n",
    "            for t in range(len(records[i][j])):\n",
    "                records[i][j][t]=random_zero(records[i][j][t])\n",
    "    return records\n",
    "\n",
    "def random_zero(records):\n",
    "    a=random.random()<0.10\n",
    "    if a:\n",
    "        return np.array([0,0,0])\n",
    "    return records\n",
    "\n",
    "\n",
    "    \n",
    "            \n",
    "def weight_variable(shape):            #가중치 생성\n",
    "    initial = tf.truncated_normal(shape, stddev = 0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):                         #편향 생성\n",
    "    initial = tf.constant(0.0, shape = shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def depthwise_conv2d(x, W):\n",
    "    return tf.nn.depthwise_conv2d(x,W, [1, 1, 1, 1], padding='VALID')\n",
    "\n",
    "def apply_depthwise_conv(x,kernel_size,num_channels,depth):#각 채널마다 활성곱 수행후 ReLu활성화 함수로 pass\n",
    "    weights = weight_variable([1, kernel_size, num_channels, depth])\n",
    "    biases = bias_variable([depth * num_channels])\n",
    "    return tf.nn.relu(tf.add(depthwise_conv2d(x, weights),biases))\n",
    "    \n",
    "def apply_max_pool(x,kernel_size,stride_size):#생성된 convolution층에 pooling 적용\n",
    "    return tf.nn.max_pool(x, ksize=[1, 1, kernel_size, 1], \n",
    "                          strides=[1, 1, stride_size, 1], padding='VALID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file name is Converted_circle_Goeun_1012.csv num is 0\n",
      "name is  Converted_circle_Goeun_1012 ext is .csv\n",
      "fileData : 3\n",
      "file name is Converted_circle_Sua_1012.csv num is 1\n",
      "name is  Converted_circle_Sua_1012 ext is .csv\n",
      "fileData : 3\n",
      "file name is Converted_circle_Sungshil_1012.csv num is 2\n",
      "name is  Converted_circle_Sungshil_1012 ext is .csv\n",
      "fileData : 3\n",
      "file name is Converted_triangle_Goeun_1012.csv num is 3\n",
      "name is  Converted_triangle_Goeun_1012 ext is .csv\n",
      "fileData : 3\n",
      "file name is Converted_triangle_Sua_1012.csv num is 4\n",
      "name is  Converted_triangle_Sua_1012 ext is .csv\n",
      "fileData : 3\n",
      "file name is Converted_triangle_Sungshil_1012.csv num is 5\n",
      "name is  Converted_triangle_Sungshil_1012 ext is .csv\n",
      "fileData : 3\n",
      "file name is Converted_X_Goeun_1012.csv num is 6\n",
      "name is  Converted_X_Goeun_1012 ext is .csv\n",
      "fileData : 3\n",
      "file name is Converted_X_Sua_1012.csv num is 7\n",
      "name is  Converted_X_Sua_1012 ext is .csv\n",
      "fileData : 3\n",
      "file name is Converted_X_Sungshil_1012.csv num is 8\n",
      "name is  Converted_X_Sungshil_1012 ext is .csv\n",
      "fileData : 3\n",
      "[43429, 44769, 43277, 45375, 43463, 43794, 43261, 43316, 43709]\n",
      "len(records) :  9\n",
      "len(labels) :  9\n",
      "**len_record :  9\n",
      "number of patterm :  109\n",
      "number of patterm :  115\n",
      "number of patterm :  101\n",
      "number of patterm :  95\n",
      "number of patterm :  79\n",
      "number of patterm :  76\n",
      "number of patterm :  93\n",
      "number of patterm :  100\n",
      "number of patterm :  98\n"
     ]
    }
   ],
   "source": [
    "(records,labels, gesture_num_records) = readData(\"dr\") #labels는 \n",
    "records_test=records\n",
    "labels_test=labels\n",
    "gesture_num_records_test=gesture_num_records\n",
    "print(gesture_num_records)\n",
    "print(\"len(records) : \", len(records))\n",
    "print(\"len(labels) : \", len(labels))\n",
    "print(\"**len_record : \",len(records))\n",
    "records, nums_pattern = splitData(records)\n",
    "records_test=records\n",
    "nums_pattern_test =nums_pattern\n",
    "\n",
    "###################################################### noise\n",
    "#records=missing_noise(records)\n",
    "records_test=missing_noise(records_test)\n",
    "#####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_labels_len : 746\n",
      "109\n",
      "115\n",
      "101\n",
      "55\n",
      "55\n",
      "28\n",
      "93\n",
      "100\n",
      "98\n",
      "sum of labels after shortenn : 754\n"
     ]
    }
   ],
   "source": [
    "len_label=len(records)\n",
    "len_label_test=len(records)\n",
    "shortened_records, new_labels = shortenRecordsLen(records, length, labels)\n",
    "shortened_records_test, new_labels_test = shortenRecordsLen(records_test, length, labels_test)\n",
    "print(\"new_labels_len :\",len(new_labels)) \n",
    "\n",
    "t=0\n",
    "for i in range(len(shortened_records)):\n",
    "    print(len(shortened_records[i]))\n",
    "    t+=len(shortened_records[i])\n",
    "print(\"sum of labels after shortenn :\",t)\n",
    "#labels = np.asarray(pd.get_dummies(labels), dtype = np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len a 9\n",
      " >> total's is  746.0\n",
      "labels is  746\n",
      "746\n",
      "labels_len : 746\n",
      "labels_dummies : [[0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " ...\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#records = np.asarray(records)\n",
    "records_1,labels=pattern_reshape(shortened_records,length,new_labels)\n",
    "records_1_test,labels_test=pattern_reshape(shortened_records_test,length,new_labels_test)\n",
    "print(\"len a\",len(shortened_records))\n",
    "print(\" >> total's is \",len(records_1)/(length*3))\n",
    "print(\"labels is \",len(labels))\n",
    "###########################################################\n",
    "num_ges=int(len(records_1)/(length*3))\n",
    "num_ges_test=int(len(records_1_test)/(length*3))\n",
    "print(num_ges_test)\n",
    "print(\"labels_len :\",len(labels))\n",
    "labels=np.asarray(pd.get_dummies(labels), dtype = np.int8)\n",
    "labels_test=np.asarray(pd.get_dummies(labels_test), dtype = np.int8)\n",
    "\n",
    "print(\"labels_dummies :\",labels)\n",
    "###########################################################\n",
    "reshaped_records = records_1.reshape(num_ges,1,length, 3) #recoed를 1차원으로 reshape\n",
    "reshaped_records_test = records_1.reshape(num_ges_test,1,length, 3) #recoed를 1차원으로 reshape\n",
    "reshaped_label=labels.reshape(len(labels), 3)\n",
    "reshaped_label_test=labels.reshape(len(labels_test), 3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split = np.random.rand(len(reshaped_records)) < 0.60\n",
    "train_x = reshaped_records[train_test_split]\n",
    "train_y = labels[train_test_split]\n",
    "test_x = reshaped_records[~train_test_split]\n",
    "test_y = labels[~train_test_split]\n",
    "\n",
    "##########################################\n",
    "train_test_split_test = np.random.rand(len(reshaped_records_test)) < 0.60\n",
    "train_x_test = reshaped_records[train_test_split_test]\n",
    "train_y_test = labels[train_test_split_test]\n",
    "test_x_test = reshaped_records[~train_test_split_test]\n",
    "test_y_test = labels[~train_test_split_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_height = 1\n",
    "input_width = length\n",
    "num_labels = 3\n",
    "num_channels = 3\n",
    "\n",
    "batch_size = 10\n",
    "kernel_size = 60 #??\n",
    "depth = 60\n",
    "num_hidden = 1000\n",
    "\n",
    "learning_rate = 0.0001\n",
    "training_epochs = 50\n",
    "\n",
    "total_batches = train_x.shape[0] // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None,input_height,input_width,num_channels])\n",
    "# input data\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "# output data\n",
    "\n",
    "c = apply_depthwise_conv(X,kernel_size,num_channels,depth) #1d convolution\n",
    "p = apply_max_pool(c,20,2) #1d Max-Pooling\n",
    "c = apply_depthwise_conv(p,6,depth*num_channels,depth//10) #1d convolution\n",
    "\n",
    "shape = c.get_shape().as_list()\n",
    "c_flat = tf.reshape(c, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "f_weights_l1 = weight_variable([shape[1] * shape[2] * depth * num_channels * (depth//10), num_hidden])\n",
    "f_biases_l1 = bias_variable([num_hidden])\n",
    "f = tf.nn.tanh(tf.add(tf.matmul(c_flat, f_weights_l1),f_biases_l1))\n",
    "\n",
    "out_weights = weight_variable([num_hidden, num_labels])\n",
    "out_biases = bias_variable([num_labels])\n",
    "y_ = tf.nn.softmax(tf.matmul(f, out_weights) + out_biases)\n",
    "# softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = -tf.reduce_sum(Y * tf.log(y_))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Training Loss:  0.1272044  Training Accuracy:  0.35955057\n",
      "Epoch:  1  Training Loss:  0.093348555  Training Accuracy:  0.35955057\n",
      "Epoch:  2  Training Loss:  0.06830351  Training Accuracy:  0.4022472\n",
      "Epoch:  3  Training Loss:  0.051079765  Training Accuracy:  0.51910114\n",
      "Epoch:  4  Training Loss:  0.039043516  Training Accuracy:  0.68988764\n",
      "Epoch:  5  Training Loss:  0.030391963  Training Accuracy:  0.7820225\n",
      "Epoch:  6  Training Loss:  0.024432834  Training Accuracy:  0.8494382\n",
      "Epoch:  7  Training Loss:  0.020093013  Training Accuracy:  0.8988764\n",
      "Epoch:  8  Training Loss:  0.016902387  Training Accuracy:  0.9258427\n",
      "Epoch:  9  Training Loss:  0.014461381  Training Accuracy:  0.941573\n",
      "Epoch:  10  Training Loss:  0.012723636  Training Accuracy:  0.9640449\n",
      "Epoch:  11  Training Loss:  0.011179248  Training Accuracy:  0.96629214\n",
      "Epoch:  12  Training Loss:  0.01010402  Training Accuracy:  0.9707865\n",
      "Epoch:  13  Training Loss:  0.009162407  Training Accuracy:  0.9730337\n",
      "Epoch:  14  Training Loss:  0.008383724  Training Accuracy:  0.9730337\n",
      "Epoch:  15  Training Loss:  0.0077851685  Training Accuracy:  0.9752809\n",
      "Epoch:  16  Training Loss:  0.0072243586  Training Accuracy:  0.9752809\n",
      "Epoch:  17  Training Loss:  0.00676864  Training Accuracy:  0.9752809\n",
      "Epoch:  18  Training Loss:  0.0063861986  Training Accuracy:  0.9752809\n",
      "Epoch:  19  Training Loss:  0.0060401  Training Accuracy:  0.9752809\n",
      "Epoch:  20  Training Loss:  0.0057578385  Training Accuracy:  0.9752809\n",
      "Epoch:  21  Training Loss:  0.005499266  Training Accuracy:  0.9775281\n",
      "Epoch:  22  Training Loss:  0.005271002  Training Accuracy:  0.98202246\n",
      "Epoch:  23  Training Loss:  0.0050693452  Training Accuracy:  0.9797753\n",
      "Epoch:  24  Training Loss:  0.0049075354  Training Accuracy:  0.98202246\n",
      "Epoch:  25  Training Loss:  0.0047274176  Training Accuracy:  0.98202246\n",
      "Epoch:  26  Training Loss:  0.00459621  Training Accuracy:  0.98202246\n",
      "Epoch:  27  Training Loss:  0.004449079  Training Accuracy:  0.98202246\n",
      "Epoch:  28  Training Loss:  0.0043193055  Training Accuracy:  0.98202246\n",
      "Epoch:  29  Training Loss:  0.0041779634  Training Accuracy:  0.98651683\n",
      "Epoch:  30  Training Loss:  0.004085408  Training Accuracy:  0.98651683\n",
      "Epoch:  31  Training Loss:  0.003983967  Training Accuracy:  0.98651683\n",
      "Epoch:  32  Training Loss:  0.0038773979  Training Accuracy:  0.98651683\n",
      "Epoch:  33  Training Loss:  0.003792537  Training Accuracy:  0.98651683\n",
      "Epoch:  34  Training Loss:  0.0037018927  Training Accuracy:  0.98876405\n",
      "Epoch:  35  Training Loss:  0.0036213268  Training Accuracy:  0.98876405\n",
      "Epoch:  36  Training Loss:  0.0035324134  Training Accuracy:  0.98876405\n",
      "Epoch:  37  Training Loss:  0.003465088  Training Accuracy:  0.98876405\n",
      "Epoch:  38  Training Loss:  0.0033902498  Training Accuracy:  0.98876405\n",
      "Epoch:  39  Training Loss:  0.0033286503  Training Accuracy:  0.98876405\n",
      "Epoch:  40  Training Loss:  0.0032585834  Training Accuracy:  0.99101126\n",
      "Epoch:  41  Training Loss:  0.0032051546  Training Accuracy:  0.99101126\n",
      "Epoch:  42  Training Loss:  0.0031449283  Training Accuracy:  0.99101126\n",
      "Epoch:  43  Training Loss:  0.0030876244  Training Accuracy:  0.99101126\n",
      "Epoch:  44  Training Loss:  0.003035389  Training Accuracy:  0.99101126\n",
      "Epoch:  45  Training Loss:  0.0029855394  Training Accuracy:  0.99101126\n",
      "Epoch:  46  Training Loss:  0.0029345572  Training Accuracy:  0.99101126\n",
      "Epoch:  47  Training Loss:  0.002893533  Training Accuracy:  0.99101126\n",
      "Epoch:  48  Training Loss:  0.0028408216  Training Accuracy:  0.99101126\n",
      "Epoch:  49  Training Loss:  0.0028017065  Training Accuracy:  0.99101126\n",
      "Testing Accuracy: 0.9767442\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session: \n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(training_epochs):\n",
    "        cost_history = np.empty(shape=[1],dtype=float)\n",
    "        for b in range(total_batches):    \n",
    "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "        for b in range(total_batches):    \n",
    "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
    "            batch_x = train_x_test[offset:(offset + batch_size), :, :, :]\n",
    "            batch_y = train_y_test[offset:(offset + batch_size), :]\n",
    "            \n",
    "            _, c = session.run([optimizer, loss],feed_dict={X: batch_x, Y : batch_y})\n",
    "            cost_history = np.append(cost_history,c)\n",
    "        print (\"Epoch: \",epoch,\" Training Loss: \",c,\" Training Accuracy: \", session.run(accuracy, feed_dict={X: train_x, Y: train_y}))\n",
    "    print (\"Testing Accuracy:\", session.run(accuracy, feed_dict={X: test_x, Y: test_y}))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
