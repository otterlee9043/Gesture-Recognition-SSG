{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "from scipy import stats\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from random import *\n",
    "\n",
    "fps = 120\n",
    "moving_avg_len = 10\n",
    "filter_value = 20\n",
    "length = 200\n",
    "threshold_pattern=0.4\n",
    "#10.18. 목\n",
    "\n",
    "\n",
    "\n",
    "def readFileData(file):\n",
    "    column_names = ['wmx1', 'wmy1', 'wmz1']\n",
    "\n",
    "    data = pd.read_csv(file, skiprows = 1 , names = column_names)\n",
    " \n",
    "    wx = data[\"wmx1\"]\n",
    "    wy = data[\"wmy1\"]\n",
    "    wz = data[\"wmz1\"]\n",
    "\n",
    "    records=movingavg(wx,wy,wz)\n",
    "    return np.dstack([wx,wy,wz])[0]\n",
    "\n",
    "def readData(directory):\n",
    "    records = []\n",
    "    labels = []\n",
    "    gesture_num_records = []\n",
    "    allFiles = glob.glob(\"*.csv\")\n",
    "    for i,file in enumerate(allFiles):\n",
    "        fileName = os.path.basename(file)\n",
    "        print(\"file name is\", fileName,\"\\nnum is\",i)\n",
    "        (name, ext) = os.path.splitext(fileName)\n",
    "        print(\"name is \",name,\"ext is\",ext)\n",
    "        parts = name.split(\"_\")\n",
    "        if (True):\n",
    "            label = parts[1] #'circle', 'triangle', 'X'\n",
    "            fileData = readFileData(file)\n",
    "            records.append(fileData)\n",
    "            #print(\"file data is \",fileData)\n",
    "            labels.append(label)\n",
    "            gesture_num_records.append(len(fileData))\n",
    "\n",
    "    return (records, labels, gesture_num_records)\n",
    "\n",
    "def movingavg(x,y,z):\n",
    "    x_avg = np.zeros(moving_avg_len)/moving_avg_len\n",
    "    y_avg = np.zeros(moving_avg_len)/moving_avg_len\n",
    "    z_avg = np.zeros(moving_avg_len)/moving_avg_len\n",
    "    x_avg = np.convolve(x,x_avg,'same')\n",
    "    y_avg = np.convolve(y,y_avg,'same')\n",
    "    z_avg = np.convolve(z,z_avg,'same')\n",
    "    return [x_avg,y_avg,z_avg]\n",
    "\n",
    "def splitData(records):##sungshil\n",
    "    \n",
    "    term=fps // 24\n",
    "    record_sum = []\n",
    "    record = []\n",
    "    record_all = []\n",
    "    sum_now = 0\n",
    "    sum_pre = 0\n",
    "    ct = 0\n",
    "    start = 0\n",
    "    end = 0\n",
    "    num_pattern=[]\n",
    "    for t in range(len(records)):\n",
    "        ct = 0\n",
    "        record_one=[]\n",
    "        for i in range((len(records[t])//(term))-100):\n",
    "            sum = 0\n",
    "            record_x = np.empty(shape=[1],dtype=float)\n",
    "            record_y = np.empty(shape=[1],dtype=float)\n",
    "            record_z = np.empty(shape=[1],dtype=float)\n",
    "            for j in range(fps//2):\n",
    "                record_x = np.append(record_x,records[t][term*i+j][0])\n",
    "                record_y = np.append(record_y,records[t][term*i+j][1])\n",
    "                record_z = np.append(record_z,records[t][term*i+j][2])\n",
    "            for k in range(fps//2 - 2):\n",
    "                sum = sum+(record_x[k]-record_x[k+1])**2+(record_y[k]-record_y[k+1])**2+(record_z[k]-record_z[k+1])**2\n",
    "\n",
    "            sum_pre = sum_now\n",
    "            sum_now = sum\n",
    "            record_sum.append(sum)\n",
    "            if (sum_pre < threshold_pattern and sum_now > threshold_pattern):\n",
    "                start = term * i\n",
    "            if (sum_pre > threshold_pattern and sum_now < threshold_pattern):\n",
    "                end = term * i  \n",
    "                s = []\n",
    "                if (end-start)>120:\n",
    "                    ct = ct + 1\n",
    "                    #print(\">> start : \",start)\n",
    "                    #print(\" >> end : \",end,\", step : \" ,ct, \", length of end-start : \",(end-start))\n",
    "                    for k in range(end-start):\n",
    "                        #t=(records[0][term*i+j+k])\n",
    "                        #print(\"t is\",t)\n",
    "                        s.append(records[t][start+k])\n",
    "                        #print(\"s is \",s)\n",
    "                    record_one.append(s)\n",
    "        record_all.append(record_one)\n",
    "        print(\"number of patterm : \",ct)\n",
    "        num_pattern.append(ct)\n",
    "        #print(\"average is\",np.average(record_sum))\n",
    "        #plt.plot(record_sum[1500:2000])\n",
    " \n",
    "    return (record_all,num_pattern)\n",
    "\n",
    "def getRecordsMaxLength(records):\n",
    "    maxLen = 0\n",
    "    for record in records:\n",
    "        if (len(record) > maxLen):\n",
    "            maxLen = len(record)\n",
    "        \n",
    "    return maxLen\n",
    "\n",
    "def shiftMarkerData(records) :\n",
    "    for records_by_file in records :\n",
    "        print(\"len(records_by_file) : \", len(records_by_file))\n",
    "        for records_xyz in records_by_file :\n",
    "            print(\"len(records_xyz) : \", len(records_xyz))\n",
    "            #print(\"BEFORE shift : \", records_xyz)\n",
    "            shifted_data = [records_xyz[0] + uniform[-0.1, 0.1], records_xyz[1] + uniform[-0.1, 0.1], records_xyz[2] + uniform[-0.1, 0.1]]\n",
    "            records_xyz = shifted_data\n",
    "            #print(\"AFTER shift : \", len(records_xyz))\n",
    "    return records\n",
    "\n",
    "def shortenRecordsLen(records, length, first_label):\n",
    "    ret1 = []\n",
    "    new_labels = []\n",
    "    for num in range(len(records)):\n",
    "        ret2 = []\n",
    "        for index in range(len(records[num])):\n",
    "            s = []\n",
    "            if (len(records[num][index])>length):\n",
    "                record = records[num][index]\n",
    "                if len(record) < 1000:\n",
    "                    new_labels.append(first_label[num])\n",
    "                    for i in range(length):\n",
    "                        ret3=[]                      \n",
    "                        t=i*(len(record)/length)\n",
    "                        if t%1 == 0:\n",
    "                            s.append(record[int(t)])\n",
    "                        else:\n",
    "                            a = t//1\n",
    "                            s1 = t-a\n",
    "                            s2 = a+1-t\n",
    "                            for j in range(3):\n",
    "                                ret3.append( s1*record[int(a)][j] + s2*record[int(a)][j] )\n",
    "                            s.append(ret3)\n",
    "                ret2.append(s)           \n",
    "                \n",
    "        ret1.append(ret2)\n",
    "    return ret1, new_labels\n",
    "\n",
    "def labeling(records, num_pattern):\n",
    "    labels = []\n",
    "    '''recods 합쳐진 다음에 num_pattern로 제스쳐 개수 계산하지 말고 records length로 바꾸기 '''\n",
    "    for i in range(num_pattern[0] + num_pattern[1] + num_pattern[2]):\n",
    "        if i < num_pattern[0] :\n",
    "            labels.append('circle')\n",
    "        elif i < num_pattern[0] + num_pattern[1]:\n",
    "            labels.append('triangle')\n",
    "        else :labels.append('X')\n",
    "    print(\"labelling - labels length : \", len(labels))\n",
    "    return labels\n",
    "\n",
    "def pattern_reshape(records,length,labels):\n",
    "    segment=[]\n",
    "    record_po=np.empty(0)\n",
    "    record_lab=[]\n",
    "    ct=0\n",
    "    for i in range(len(records)):\n",
    "        for j in range(len(records[i])): # num of gesture 횟수만큼\n",
    "            length_ary=[]\n",
    "            for k in range(len(records[i][j])): # 150회만큼\n",
    "                x=np.array(records[i][j][k][0]) # files X num of gesture X 150 X 3\n",
    "                y=np.array(records[i][j][k][1])\n",
    "                z=np.array(records[i][j][k][2])\n",
    "                length_ary.append([x,y,z])\n",
    "            if len(records[i][j])==length:\n",
    "                segment.append([length_ary,labels[ct]])\n",
    "                ct+=1\n",
    "\n",
    "    for k in range(len(segment)):\n",
    "        record_lab.append(segment[k][1])\n",
    "        for t in range(len(segment[k][0])): # x,y,z\n",
    "            record_po=np.append(record_po,segment[k][0][t])\n",
    "            \n",
    "            \n",
    "    return record_po,record_lab\n",
    "            \n",
    "def weight_variable(shape):#가중치 생성\n",
    "    initial = tf.truncated_normal(shape, stddev = 0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):#편향 생성\n",
    "    initial = tf.constant(0.0, shape = shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def depthwise_conv2d(x, W):\n",
    "    return tf.nn.depthwise_conv2d(x,W, [1, 1, 1, 1], padding='VALID')\n",
    "\n",
    "def apply_depthwise_conv(x,kernel_size,num_channels,depth):#각 채널마다 활성곱 수행후 ReLu활성화 함수로 pass\n",
    "    weights = weight_variable([1, kernel_size, num_channels, depth])\n",
    "    biases = bias_variable([depth * num_channels])\n",
    "    return tf.nn.relu(tf.add(depthwise_conv2d(x, weights),biases))\n",
    "    \n",
    "def apply_max_pool(x,kernel_size,stride_size):#생성된 convolution층에 pooling 적용\n",
    "    return tf.nn.max_pool(x, ksize=[1, 1, kernel_size, 1], \n",
    "                          strides=[1, 1, stride_size, 1], padding='VALID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file name is Converted_circle_Sua_1012.csv \n",
      "num is 0\n",
      "name is  Converted_circle_Sua_1012 ext is .csv\n",
      "file name is Converted_triangle_Sua_1012.csv \n",
      "num is 1\n",
      "name is  Converted_triangle_Sua_1012 ext is .csv\n",
      "file name is Converted_X_Sungshil_1012.csv \n",
      "num is 2\n",
      "name is  Converted_X_Sungshil_1012 ext is .csv\n",
      "[44769, 43463, 43709]\n",
      "len(records) :  3\n",
      "len(labels) :  3\n",
      "**len_record :  3\n",
      "number of patterm :  116\n",
      "number of patterm :  79\n",
      "number of patterm :  98\n",
      "#####################\n"
     ]
    }
   ],
   "source": [
    "(records,labels, gesture_num_records) = readData(\"dr\") #labels는 \n",
    "print(gesture_num_records)\n",
    "print(\"len(records) : \", len(records))\n",
    "print(\"len(labels) : \", len(labels))\n",
    "print(\"**len_record : \",len(records))\n",
    "records, nums_pattern = splitData(records)\n",
    "print(\"#####################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(records_by_file) :  116\n",
      "len(records_xyz) :  185\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'method' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-1d5cc143d590>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mshifted_records\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshiftMarkerData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-5e0308257424>\u001b[0m in \u001b[0;36mshiftMarkerData\u001b[0;34m(records)\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"len(records_xyz) : \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords_xyz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[1;31m#print(\"BEFORE shift : \", records_xyz)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0mshifted_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mrecords_xyz\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0muniform\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecords_xyz\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0muniform\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecords_xyz\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0muniform\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m             \u001b[0mrecords_xyz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshifted_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[1;31m#print(\"AFTER shift : \", len(records_xyz))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'method' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "shifted_records = shiftMarkerData(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_labels_len : 746\n",
      "109\n",
      "115\n",
      "101\n",
      "55\n",
      "55\n",
      "28\n",
      "93\n",
      "100\n",
      "98\n",
      "sum of labels after shortenn : 754\n"
     ]
    }
   ],
   "source": [
    "len_label=len(records)\n",
    "\n",
    "shortened_records, new_labels = shortenRecordsLen(records, length, labels)\n",
    "print(\"new_labels_len :\",len(new_labels)) \n",
    "\n",
    "t=0\n",
    "for i in range(len(shortened_records)):\n",
    "    print(len(shortened_records[i]))\n",
    "    t+=len(shortened_records[i])\n",
    "print(\"sum of labels after shortenn :\",t)\n",
    "#labels = np.asarray(pd.get_dummies(labels), dtype = np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len a 9\n",
      " >> total's is  746.0\n",
      "labels is  746\n",
      "labels : ['circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'circle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'triangle', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X']\n",
      "labels_len : 746\n",
      "labels_dummies : [[0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " ...\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#records = np.asarray(records)\n",
    "records_1,labels=pattern_reshape(shortened_records,length,new_labels)\n",
    "print(\"len a\",len(shortened_records))\n",
    "print(\" >> total's is \",len(records_1)/(length*3))\n",
    "print(\"labels is \",len(labels))\n",
    "###########################################################tem\n",
    "num_ges=int(len(records_1)/(length*3))\n",
    "\n",
    "print(\"labels_len :\",len(labels))\n",
    "labels=np.asarray(pd.get_dummies(labels), dtype = np.int8)\n",
    "print(\"labels_dummies :\",labels)\n",
    "#################################              \n",
    "reshaped_records = records_1.reshape(num_ges,1,length, 3) #recoed를 1차원으로 reshape\n",
    "reshaped_label=labels.reshape(len(labels), 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_test_split = np.random.rand(len(reshaped_records)) < 0.60\n",
    "train_x = reshaped_records[train_test_split]\n",
    "train_y = labels[train_test_split]\n",
    "test_x = reshaped_records[~train_test_split]\n",
    "test_y = labels[~train_test_split]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_height = 1\n",
    "input_width = length\n",
    "num_labels = 3\n",
    "num_channels = 3\n",
    "\n",
    "batch_size = 10\n",
    "kernel_size = 60 #??\n",
    "depth = 60\n",
    "num_hidden = 1000\n",
    "\n",
    "learning_rate = 0.0001\n",
    "training_epochs = 50\n",
    "\n",
    "total_batches = train_x.shape[0] // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None,input_height,input_width,num_channels])\n",
    "# input data\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "# output data\n",
    "\n",
    "c = apply_depthwise_conv(X,kernel_size,num_channels,depth) #1d convolution\n",
    "p = apply_max_pool(c,20,2) #1d Max-Pooling\n",
    "c = apply_depthwise_conv(p,6,depth*num_channels,depth//10) #1d convolution\n",
    "\n",
    "shape = c.get_shape().as_list()\n",
    "c_flat = tf.reshape(c, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "f_weights_l1 = weight_variable([shape[1] * shape[2] * depth * num_channels * (depth//10), num_hidden])\n",
    "f_biases_l1 = bias_variable([num_hidden])\n",
    "f = tf.nn.tanh(tf.add(tf.matmul(c_flat, f_weights_l1),f_biases_l1))\n",
    "\n",
    "out_weights = weight_variable([num_hidden, num_labels])\n",
    "out_biases = bias_variable([num_labels])\n",
    "y_ = tf.nn.softmax(tf.matmul(f, out_weights) + out_biases)\n",
    "# softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = -tf.reduce_sum(Y * tf.log(y_))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Training Loss:  32.521248  Training Accuracy:  0.3883721\n",
      "Epoch:  1  Training Loss:  22.132828  Training Accuracy:  0.63255817\n",
      "Epoch:  2  Training Loss:  15.121701  Training Accuracy:  0.80697674\n",
      "Epoch:  3  Training Loss:  10.293707  Training Accuracy:  0.8627907\n",
      "Epoch:  4  Training Loss:  7.0963883  Training Accuracy:  0.89534885\n",
      "Epoch:  5  Training Loss:  5.0251174  Training Accuracy:  0.9209302\n",
      "Epoch:  6  Training Loss:  3.6613398  Training Accuracy:  0.9302326\n",
      "Epoch:  7  Training Loss:  2.7314918  Training Accuracy:  0.9348837\n",
      "Epoch:  8  Training Loss:  2.114379  Training Accuracy:  0.9488372\n",
      "Epoch:  9  Training Loss:  1.6457703  Training Accuracy:  0.96046513\n",
      "Epoch:  10  Training Loss:  1.307996  Training Accuracy:  0.96511626\n",
      "Epoch:  11  Training Loss:  1.062242  Training Accuracy:  0.96976745\n",
      "Epoch:  12  Training Loss:  0.8741854  Training Accuracy:  0.96976745\n",
      "Epoch:  13  Training Loss:  0.74162817  Training Accuracy:  0.97209305\n",
      "Epoch:  14  Training Loss:  0.62852895  Training Accuracy:  0.9744186\n",
      "Epoch:  15  Training Loss:  0.5434314  Training Accuracy:  0.9744186\n",
      "Epoch:  16  Training Loss:  0.47273645  Training Accuracy:  0.9744186\n",
      "Epoch:  17  Training Loss:  0.41574726  Training Accuracy:  0.9744186\n",
      "Epoch:  18  Training Loss:  0.37074885  Training Accuracy:  0.9767442\n",
      "Epoch:  19  Training Loss:  0.32952917  Training Accuracy:  0.97906977\n",
      "Epoch:  20  Training Loss:  0.30022055  Training Accuracy:  0.97906977\n",
      "Epoch:  21  Training Loss:  0.27087474  Training Accuracy:  0.98139536\n",
      "Epoch:  22  Training Loss:  0.24642815  Training Accuracy:  0.98372096\n",
      "Epoch:  23  Training Loss:  0.22540455  Training Accuracy:  0.98372096\n",
      "Epoch:  24  Training Loss:  0.20810485  Training Accuracy:  0.98372096\n",
      "Epoch:  25  Training Loss:  0.19243592  Training Accuracy:  0.98372096\n",
      "Epoch:  26  Training Loss:  0.17683274  Training Accuracy:  0.98372096\n",
      "Epoch:  27  Training Loss:  0.1640391  Training Accuracy:  0.98372096\n",
      "Epoch:  28  Training Loss:  0.15365034  Training Accuracy:  0.98372096\n",
      "Epoch:  29  Training Loss:  0.14273216  Training Accuracy:  0.9883721\n",
      "Epoch:  30  Training Loss:  0.13428444  Training Accuracy:  0.9883721\n",
      "Epoch:  31  Training Loss:  0.1251297  Training Accuracy:  0.9883721\n",
      "Epoch:  32  Training Loss:  0.11748502  Training Accuracy:  0.9883721\n",
      "Epoch:  33  Training Loss:  0.1107598  Training Accuracy:  0.9883721\n",
      "Epoch:  34  Training Loss:  0.104843035  Training Accuracy:  0.9883721\n",
      "Epoch:  35  Training Loss:  0.09943212  Training Accuracy:  0.9883721\n",
      "Epoch:  36  Training Loss:  0.09433185  Training Accuracy:  0.9883721\n",
      "Epoch:  37  Training Loss:  0.089623645  Training Accuracy:  0.9883721\n",
      "Epoch:  38  Training Loss:  0.085286126  Training Accuracy:  0.9883721\n",
      "Epoch:  39  Training Loss:  0.080943234  Training Accuracy:  0.9883721\n",
      "Epoch:  40  Training Loss:  0.077558294  Training Accuracy:  0.9883721\n",
      "Epoch:  41  Training Loss:  0.07384513  Training Accuracy:  0.9883721\n",
      "Epoch:  42  Training Loss:  0.07095273  Training Accuracy:  0.9883721\n",
      "Epoch:  43  Training Loss:  0.06814806  Training Accuracy:  0.9883721\n",
      "Epoch:  44  Training Loss:  0.06531834  Training Accuracy:  0.9883721\n",
      "Epoch:  45  Training Loss:  0.062686116  Training Accuracy:  0.9883721\n",
      "Epoch:  46  Training Loss:  0.060246427  Training Accuracy:  0.9883721\n",
      "Epoch:  47  Training Loss:  0.05813434  Training Accuracy:  0.9883721\n",
      "Epoch:  48  Training Loss:  0.0556577  Training Accuracy:  0.9883721\n",
      "Epoch:  49  Training Loss:  0.05378015  Training Accuracy:  0.9883721\n",
      "Testing Accuracy: 0.98734176\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session: \n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(training_epochs):\n",
    "        cost_history = np.empty(shape=[1],dtype=float)\n",
    "        for b in range(total_batches):    \n",
    "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([optimizer, loss],feed_dict={X: batch_x, Y : batch_y})\n",
    "            cost_history = np.append(cost_history,c)\n",
    "        print (\"Epoch: \",epoch,\" Training Loss: \",c,\" Training Accuracy: \", session.run(accuracy, feed_dict={X: train_x, Y: train_y}))\n",
    "    print (\"Testing Accuracy:\", session.run(accuracy, feed_dict={X: test_x, Y: test_y}))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
